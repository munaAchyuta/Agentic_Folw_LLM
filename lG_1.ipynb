{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4931ac25-99f9-4f04-b3d1-4683f7853667",
   "metadata": {},
   "source": [
    "# Install Required Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "568c84d6-9df6-4b7b-b50d-476c0a64a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -U --quiet langchain_community tiktoken langchain-mistralai langchainhub chromadb langchain langgraph tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0642cb96",
   "metadata": {},
   "source": [
    "## Define Functions to build data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6e984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, List\n",
    "import json\n",
    "\n",
    "# Define the system message and context\n",
    "SYSTEM_MESSAGE = \"\"\"You are an AI assistant tasked with analyzing function definition and it's dependencies to determine if sufficient information is available for test code generation.\"\"\"\n",
    "\n",
    "FUNCTION_CONTEXT = \"\"\"\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b63d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):\n",
    "    '''\n",
    "    NEED TO BE IMPLEMENTED.\n",
    "    '''\n",
    "    response = prompt\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14f9ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOBE DONE: clone repo\n",
    "\n",
    "# TOBE DONE: get schema information w.r.t module. below cell is example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55238a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Index having module's schema information.\n",
    "module_meta_schema = {'app.py':\n",
    "                      {'abs_path':'path to file'\n",
    "                      ,'function_name':{'definition':'definition goes here','description':'objective goes here..'\n",
    "                                        ,'params':[{'type':'dict/list/str/int','data':'data sample if any'},]\n",
    "                                        ,'returns':{'type':'dict/list/str/int','data':'data sample if any'}\n",
    "                                        ,'caller_function':'caller function/method name goes here'\n",
    "                                        ,\"dependency_from_body\": []\n",
    "                                        ,\"dependency_from_input\": []\n",
    "                                        }\n",
    "                      ,'class_name':{'definition':'class definition goes here'\n",
    "                                     ,'method_name':{'definition':'definition goes here','description':'objective goes here'\n",
    "                                                     ,'params':[{'type':'dict/list/str/int','data':'data sample if any'},]\n",
    "                                                     ,'returns':{'type':'dict/list/str/int','data':'data sample if any'}\n",
    "                                                     ,'caller_function':'caller function/method name goes here'\n",
    "                                                     ,\"dependency_from_body\": []\n",
    "                                                     ,\"dependency_from_input\": []\n",
    "                                                     }\n",
    "                                     },\n",
    "                      },\n",
    "                      'car_emi.py':\n",
    "                       {'abs_path':'path to file'\n",
    "                        ,'function_name':{'definition':'definition goes here','description':'objective goes here..'\n",
    "                                          ,'params':[{'type':'dict/list/str/int','data':'data sample if any'},]\n",
    "                                          ,'returns':{'type':'dict/list/str/int','data':'data sample if any'}\n",
    "                                          ,'caller_function':'caller function/method name goes here'\n",
    "                                          ,\"dependency_from_body\": []\n",
    "                                          ,\"dependency_from_input\": []\n",
    "                                          }\n",
    "                        ,'class_name':{'definition':'definition goes here'\n",
    "                                       ,'method_name':{'definition':'definition goes here','description':'objective goes here'\n",
    "                                                       ,'params':[{'type':'dict/list/str/int','data':'data sample if any'},]\n",
    "                                                       ,'returns':{'type':'dict/list/str/int','data':'data sample if any'}\n",
    "                                                       ,'caller_function':'caller function/method name goes here'\n",
    "                                                       ,\"dependency_from_body\": []\n",
    "                                                       ,\"dependency_from_input\": []\n",
    "                                                       }\n",
    "                                     },\n",
    "                        },\n",
    "                    }\n",
    "\n",
    "# TOBE DONE: get in hierarchical order of dependencies for ease of retrieval.\n",
    "\n",
    "def get_dependencies(attribute_name):\n",
    "    '''\n",
    "     attribute_name: function name or module name or class name\n",
    "     return: text having all dependencies\n",
    "    '''\n",
    "    return module_meta_schema.find(attribute_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87194a1b-535a-4593-ab95-5736fae176d1",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b3945f-ef0f-458d-a443-f763903550b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List, Optional\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        function_context: given function definition as input context\n",
    "        test_code: LLM generation\n",
    "        additional_info: get additional information from user\n",
    "        additional_info_rqd: does additional information required\n",
    "        greeting: say bye when encounters max loop limit\n",
    "        counter: keep track of numer of iteration \n",
    "    \"\"\"\n",
    "    function_context : str\n",
    "    test_code : str\n",
    "    additional_info_rqd : str\n",
    "    additional_info : str\n",
    "    greeting: Optional[str] = None\n",
    "    counter : int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd639c5-82e2-45e6-a94a-6a4039646ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "### Nodes Definition goes here\n",
    "def generate_test_code(state):\n",
    "    print(\"Generate test code:::::\")\n",
    "    function_context = state[\"function_context\"]\n",
    "    additional_info = state[\"additional_info\"]\n",
    "    \n",
    "    prompt = f\"{SYSTEM_MESSAGE}\\n\\nFunction and Dependencies:\\n{function_context}\\n\"\n",
    "    if 'additional_info' in state:\n",
    "        prompt += f\"\\nAdditional Information:\\n{additional_info}\\n\"\n",
    "    prompt += \"\\nGenerate test scenarios and code for the given function:\"\n",
    "    \n",
    "    llm_response = get_llm_response(prompt)\n",
    "    state['test_code'] = llm_response\n",
    "\n",
    "    return state\n",
    "\n",
    "def check_test_info(state):\n",
    "    print(\"Check whether all information given to go for generating test code::::::\")\n",
    "    function_context = state[\"function_context\"]\n",
    "    additional_info = state[\"additional_info\"]\n",
    "    counter = state[\"counter\"]\n",
    "    if counter is None:\n",
    "        counter = 0\n",
    "    \n",
    "    prompt = f\"{SYSTEM_MESSAGE}\\n\\nFunction and Dependencies:\\n{function_context}\\n\\nAnalyze and answer the following questions (yes/no):\"\n",
    "    questions = [\n",
    "        \"Can input/output data structures be identified to create test samples?\",\n",
    "        \"Is there sufficient clear information to create test mocks for dependencies present in body of function/method?\"\n",
    "    ]\n",
    "    \n",
    "    llm_response = get_llm_response(prompt + \"\\n\".join(questions))\n",
    "    \n",
    "    # Parse LLM response into yes/no answers\n",
    "    answers = [line.split(\":\")[-1].strip().lower() for line in llm_response.split(\"\\n\") if line]\n",
    "    #response = {q: a for q, a in zip(questions, answers)}\n",
    "    response = {q: 'yes' for q, a in zip(questions, answers)}\n",
    "    \n",
    "    state['additional_info_rqd'] = response\n",
    "    state['counter'] = counter + 1\n",
    "    \n",
    "    return state \n",
    "\n",
    "def get_user_input(state):\n",
    "    print(\"Additional information needed. Please provide::::::::\")\n",
    "    for question, answer in state['additional_info_rqd'].items():\n",
    "        if answer == 'no':\n",
    "            user_input = input(f\"{question}: \")\n",
    "            additional_info = \"\" if state.get('additional_info', \"\") is None else state.get('additional_info', \"\")\n",
    "            state['additional_info_rqd'][question] = \"yes\"\n",
    "            state['additional_info'] = additional_info + f\"\\n{question}: {user_input}\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "def check_user_input(state):\n",
    "    print(\"Decide based on user input::::::::\")\n",
    "    counter = state['counter']\n",
    "\n",
    "    if counter >= 5:\n",
    "        print(\"---DECISION: generate test code ended with counter exceed---\")\n",
    "        return \"bye\"\n",
    "    else:\n",
    "        print(\"---DECISION: check again whether all information given to go for generating test code::::::\")\n",
    "        return \"check_test_info\"\n",
    "\n",
    "def bye(state):\n",
    "    return {\"greeting\":\"The graph has finished\"}\n",
    "\n",
    "### Edges Definition goes here\n",
    "def decide_to_generate(state):\n",
    "    print(\"Decide to generate or ask user::::::\")\n",
    "    \n",
    "    if all([True if i.lower()=='yes' else False for i in state['additional_info_rqd'].values()]):\n",
    "        print(\"---DECISION: all information given to go for generating test code::::::::\")\n",
    "        return \"generate_test_code\"\n",
    "    else:\n",
    "        print(\"---DECISION: take user input---\")\n",
    "        return \"get_user_input\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa076e90-7132-4fcf-8507-db5990314c4f",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9adc1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"check_test_info\", check_test_info)\n",
    "workflow.add_node(\"get_user_input\", get_user_input)\n",
    "workflow.add_node(\"generate_test_code\", generate_test_code)\n",
    "workflow.add_node(\"bye\", bye)\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"check_test_info\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_test_info\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"get_user_input\": \"get_user_input\",\n",
    "        \"generate_test_code\": \"generate_test_code\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"get_user_input\",\n",
    "    check_user_input,\n",
    "    {\n",
    "        \"check_test_info\": \"check_test_info\",\n",
    "        \"bye\": \"bye\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"generate_test_code\", END)\n",
    "workflow.add_edge('bye', END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f51afa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_context': <langgraph.channels.last_value.LastValue at 0x1f8e6fd6a20>,\n",
       " 'test_code': <langgraph.channels.last_value.LastValue at 0x1f8e6fd6b70>,\n",
       " 'additional_info_rqd': <langgraph.channels.last_value.LastValue at 0x1f8e6fd5820>,\n",
       " 'additional_info': <langgraph.channels.last_value.LastValue at 0x1f8e739fcb0>,\n",
       " 'greeting': <langgraph.channels.last_value.LastValue at 0x1f8e7db7a70>,\n",
       " 'counter': <langgraph.channels.last_value.LastValue at 0x1f8e7db5760>,\n",
       " '__start__': <langgraph.channels.ephemeral_value.EphemeralValue at 0x1f8e7db78f0>,\n",
       " 'check_test_info': <langgraph.channels.ephemeral_value.EphemeralValue at 0x1f8e7cefb30>,\n",
       " 'get_user_input': <langgraph.channels.ephemeral_value.EphemeralValue at 0x1f8e7e2f710>,\n",
       " 'generate_test_code': <langgraph.channels.ephemeral_value.EphemeralValue at 0x1f8e7e2f770>,\n",
       " 'bye': <langgraph.channels.ephemeral_value.EphemeralValue at 0x1f8e7e2f590>,\n",
       " 'start:check_test_info': <langgraph.channels.ephemeral_value.EphemeralValue at 0x1f8e7e2dfd0>,\n",
       " 'branch:check_test_info:decide_to_generate:get_user_input': <langgraph.channels.ephemeral_value.EphemeralValue at 0x1f8e7e2e270>,\n",
       " 'branch:check_test_info:decide_to_generate:generate_test_code': <langgraph.channels.ephemeral_value.EphemeralValue at 0x1f8e7e2d9a0>,\n",
       " 'branch:get_user_input:check_user_input:check_test_info': <langgraph.channels.ephemeral_value.EphemeralValue at 0x1f8e7e2dd60>,\n",
       " 'branch:get_user_input:check_user_input:bye': <langgraph.channels.ephemeral_value.EphemeralValue at 0x1f8e7e2e5d0>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify Control Flow\n",
    "app.channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692be4f",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5b7c2fe-1fc7-4b76-bf93-ba701a40aa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check whether all information given to go for generating test code::::::\n",
      "Decide to generate or ask user::::::\n",
      "---DECISION: all information given to go for generating test code::::::::\n",
      "'Finished running: check_test_info:'\n",
      "Generate test code:::::\n",
      "'Finished running: generate_test_code:'\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"function_context\": FUNCTION_CONTEXT}\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "\n",
    "if value.get('generation',None):\n",
    "    pprint(value[\"generation\"])\n",
    "else:\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ddcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
